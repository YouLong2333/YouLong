{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\software\\anoconda\\anaconda\\mywhl\\GetOldTweets-python-master\n"
     ]
    }
   ],
   "source": [
    "cd D:\\software\\anoconda\\anaconda\\mywhl\\GetOldTweets-python-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/longyou/Documents/GetOldTweets-python-master\n"
     ]
    }
   ],
   "source": [
    "cd /Users/longyou/Documents/GetOldTweets-python-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package got3 github repository: https://github.com/Jefferson-Henrique/GetOldTweets-python\n",
    "import csv\n",
    "import os\n",
    "from os import path\n",
    "import datetime\n",
    "import got3 as got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/longyou/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/longyou/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #Natural Language Toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet scrapy\n",
    "def main(output_filename,dateSince,dateUntil,query,maxTweets):\n",
    "    def printTweet(descr, t):\n",
    "        print(descr)\n",
    "        # print(\"Username: %s\" % t.username.encode(\"utf-8\"))\n",
    "        print(\"Date: %s\" % t.date)\n",
    "        print(\"Retweets: %d\" % t.retweets)\n",
    "        print(\"Text: %s\" % t.text.encode(\"utf-8\"))\n",
    "        # print(\"geo: %s\" % t.geo)\n",
    "        # print(\"Mentions: %s\" % t.mentions.encode(\"utf-8\"))\n",
    "        # print(\"Hashtags: %s\\n\" % t.hashtags.encode(\"utf-8\"))\n",
    "        ############### parameters #################\n",
    "#     location = 'Canada'\n",
    "#     distance = '1000 meters'\n",
    "    # https://www.hko.gov.hk/wxinfo/currwx/tc_pastpos_1829.htm\n",
    "#     output_filename = \"Linfa_%s.csv\" % dateSince \n",
    "#     output_filename = \"Mangkhut.csv\"\n",
    "#     query = \"Mangkhut\"\n",
    "#     dateSince = \"2018-09-14\"\n",
    "#     dateUntil = \"2018-09-18\"\n",
    "#     maxTweets = 10000\n",
    "    \n",
    "#     output_filename = \"yutu.csv\"\n",
    "#     query = \"yutu\"\n",
    "#     dateSince = \"2018-10-24\"\n",
    "#     dateUntil = \"2018-11-03\"\n",
    "#     maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"kong_rey.csv\"\n",
    "    # query = \"kongrey typhoon\"\n",
    "    # dateSince = \"2018-09-29\"\n",
    "    # dateUntil = \"2018-10-06\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"trami.csv\"\n",
    "    # query = \"trami  typhoon\".\t\n",
    "    # dateSince = \"2018-09-21\"\n",
    "    # dateUntil = \"2018-09-30\"\n",
    "    # maxTweets = 10000#10000\n",
    "\n",
    "    #output_filename = \"mangkhut20180908-0920.csv\"\n",
    "    #query = \"mangkhut hong kong typhoon\"\n",
    "    #dateSince = \"2018-09-08\"\n",
    "    #dateUntil = \"2018-09-20\"\n",
    "    #maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"barijat.csv\"\n",
    "    # query = \"barijat typhoon\"\n",
    "    # dateSince = \"2018-09-10\"\n",
    "    # dateUntil = \"2018-09-13\"\n",
    "    # maxTweets = 10000\n",
    "    \n",
    "    # output_filename = \"jebi.csv\"\n",
    "    # query = \"jebi typhoon\"\n",
    "    # dateSince = \"2018-08-31\"\n",
    "    # dateUntil = \"2018-09-04\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    #output_filename = \"cimaron.csv\"\n",
    "    #query = \"cimaron typhoon\"\n",
    "    #dateSince = \"2018-08-20\"\n",
    "    #dateUntil = \"2018-08-23\"\n",
    "    #maxTweets = 1000\n",
    "\n",
    "    #output_filename = \"soulik.csv\"\n",
    "    #query = \"soulik typhoon\"\n",
    "    #dateSince = \"2018-08-16\"\n",
    "    #dateUntil = \"2018-08-23\"\n",
    "    #maxTweets = 10000\n",
    "\n",
    "    #output_filename = \"rumbia.csv\"\n",
    "    #query = \"rumbia typhoon\"\n",
    "    #dateSince = \"2018-08-18\"\n",
    "    #dateUntil = \"2018-08-15\"\n",
    "    #maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"leepi.csv\"\n",
    "    # query = \"leepi typhoon\"\n",
    "    # dateSince = \"2018-08-15\"\n",
    "    # dateUntil = \"2018-08-11\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"yagi.csv\"\n",
    "    # query = \"yagi typhoon\"\n",
    "    # dateSince = \"2018-08-15\"\n",
    "    # dateUntil = \"2018-08-11\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"son_tinh.csv\"\n",
    "    # query = \"son tinh typhoon\"\n",
    "    # dateSince = \"2018-10-15\"\n",
    "    # dateUntil = \"2018-11-09\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"ewiniar.csv\"\n",
    "    # query = \"ewiniar typhoon\"\n",
    "    # dateSince = \"2018-10-15\"\n",
    "    # dateUntil = \"2018-11-09\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"khanun.csv\"\n",
    "    # query = \"khanun typhoon\"\n",
    "    # dateSince = \"2017-10-14\"\n",
    "    # dateUntil = \"2017-10-15\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"mawar.csv\"\n",
    "    # query = \"mawar typhoon\"\n",
    "    # dateSince = \"2017-09-02\"\n",
    "    # dateUntil = \"2017-09-04\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"pakhar.csv\"\n",
    "    # query = \"pakhar typhoon\"\n",
    "    # dateSince = \"2017-08-26\"\n",
    "    # dateUntil = \"2017-08-27\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"hato.csv\"\n",
    "    # query = \"hato typhoon\"\n",
    "    # dateSince = \"2017-08-22\"\n",
    "    # dateUntil = \"2017-08-23\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"roke.csv\"\n",
    "    # query = \"roke typhoon\"\n",
    "    # dateSince = \"2017-07-01\"\n",
    "    # dateUntil = \"2017-07-31\"\n",
    "    # maxTweets = 10000\n",
    "\n",
    "    # output_filename = \"merbok.csv\"\n",
    "    # query = \"merbok typhoon\"\n",
    "    # dateSince = \"2017-06-01\"\n",
    "    # dateUntil = \"2017-06-30\"\n",
    "    # maxTweets = 10000\n",
    "    ############### search twitter #################\n",
    "    print(\"Searching...\")\n",
    "    # tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query).setSince(dateSince).setUntil(dateUntil).setNear(location).setWithin(distance).setMaxTweets(maxTweets)\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query).setSince(dateSince).setUntil(dateUntil).setMaxTweets(maxTweets)\n",
    "\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    # tweet = got.manager.TweetManager.getTweets(tweetCriteria)[0]\n",
    "    # printTweet(\"Get tweets by query search:\", tweet)\n",
    "\n",
    "    ############### export csv file #################\n",
    "    csv_output = [['no.','date','retweets','text']]\n",
    "    for c, tweet in enumerate(tweets, start=1):\n",
    "        if tweet.retweets >= 0:\n",
    "            strDescr = str(c) + \". Get tweets by query search:\"\n",
    "            # printTweet(strDescr, tweet)\n",
    "            csv_output.append([c, tweet.date, tweet.retweets, tweet.text.encode(\"utf-8\")])\n",
    "\n",
    "    with open('/Users/longyou/Desktop/' + output_filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerows(csv_output)\n",
    "\n",
    "    print(\"Output file:\", output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text cleaning --- very important!\n",
    "def clean_text(query):\n",
    "    #load text\n",
    "#     d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "    d = 'D:\\\\HKU\\\\Project for STAT7008\\\\data\\\\'\n",
    "    text = open(path.join(d, query + '.csv')).read()\n",
    "\n",
    "    # # split into words by white space\n",
    "    # words = text.split()\n",
    "    # # remove punctuation from each word\n",
    "    # import string\n",
    "    # table = str.maketrans('', '', string.punctuation)\n",
    "    # stripped = [w.translate(table) for w in words]\n",
    "    # print(stripped)\n",
    "\n",
    "    ##update nltk database\n",
    "    # import nltk\n",
    "    # nltk.download()\n",
    "\n",
    "    # split into words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('b')\n",
    "    stop_words.add('bi')\n",
    "    stop_words.add('n')\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # print(words[:100])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     scrapy_from = \"2018-09-14\"\n",
    "#     scrapy_to = \"2018-09-17\"\n",
    "#     while True:\n",
    "#         main(scrapy_from)\n",
    "#         scrapy_from_D = datetime.datetime.strptime(scrapy_from, \"%Y-%m-%d\")\n",
    "#         scrapy_from_D = scrapy_from_D + datetime.timedelta(days=1)\n",
    "#         scrapy_to_D = datetime.datetime.strptime(scrapy_to, \"%Y-%m-%d\")\n",
    "#         if scrapy_from_D > scrapy_to_D:\n",
    "#             break\n",
    "#         else: \n",
    "#             scrapy_from = scrapy_to_D.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "#     # clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: Mangkhut.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"Mangkhut.csv\",\"2018-09-08\",\"2018-09-20\",\"Mangkhut\",130000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: yutu.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"yutu.csv\",\"2018-10-31\",\"2018-11-02\",\"yutu\",maxTweets = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: barijat.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"barijat.csv\",\"2018-09-11\",\"2018-09-13\",\"barijat\",maxTweets = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: bebinca.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"bebinca.csv\",\"2018-08-13\",\"2018-08-16\",\"bebinca\",maxTweets = 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: son-tinh.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"son-tinh.csv\",\"2018-07-17\",\"2018-07-23\",\"son tinh\",maxTweets = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: ewiniar.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"ewiniar.csv\",\"2018-06-08\",\"2018-06-09\",\"ewiniar\",maxTweets = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: khanun.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"khanun.csv\",\"2017-10-13\",\"2017-10-16\",\"khanun\",maxTweets = 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: marwar.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"marwar.csv\",\"2017-09-02\",\"2017-09-04\",\"marwar\",maxTweets = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: pahkar.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"pahkar.csv\",\"2018-08-25\",\"2018-08-27\",\"pahkar\",maxTweets = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: hato.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"hato.csv\",\"2017-08-22\",\"2018-08-23\",\"hato\",maxTweets = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: merbok.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"merbok.csv\",\"2017-06-11\",\"2017-06-12\",\"merbok\",maxTweets = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: haima.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"haima.csv\",\"2016-06-11\",\"2017-06-12\",\"haima\",maxTweets = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: nida.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"nida.csv\",\"2016-07-30\",\"2017-08-01\",\"nida\",maxTweets = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching...\n",
      "Output file: linfa.csv\n"
     ]
    }
   ],
   "source": [
    "main(\"linfa.csv\",\"2015-07-09\",\"2015-07-10\",\"linfa\",maxTweets = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
